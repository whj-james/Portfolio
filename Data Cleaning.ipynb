{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8tinZOUlDER"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "# Data Cleaning for Restaurant online orders\n",
    "    \n",
    "#### Name: James Wang\n",
    "\n",
    "\n",
    "Date: 30/08/2022\n",
    "\n",
    "Environment: Python 3.9.7\n",
    "\n",
    "Libraries used:\n",
    "* os (for interacting with the operating system, included in Python 3.9) \n",
    "* re (for extracting pid and review for text,installed and imported)\n",
    "* pandas 1.1.0 (for dataframe, installed and imported) \n",
    "* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package) \n",
    "* itertools (for performing operations on iterables)\n",
    "* nltk 3.5 (Natural Language Toolkit, installed and imported)\n",
    "* nltk.collocations (for finding bigrams, installed and imported)\n",
    "* nltk.tokenize (for tokenization, installed and imported)\n",
    "* nltk.stem (for stemming the tokens, installed and imported)\n",
    "* sklearn.feature_extraction.text (for creating count vector,installed and imported)\n",
    "* pdfminer(for reading pdf file,installed and imported)\n",
    "* io (for trasfer pdf to text format,installed and imported)\n",
    "* math (for calculate threshold,installed and imported)\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnnLnFnLlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>\n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Input File](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Whatever else](#whetev) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Finding First 200 Meaningful Bigrams](#bigrams) <br>\n",
    "$\\;\\;\\;\\;$[4.4. Whatever else](#whetev1) <br>\n",
    "[5. Writing Output Files](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
    "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8mo6PPRlDEU"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewZrff73lDEV"
   },
   "source": [
    "This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing **pids** and **reviews**. First, we need to read the pdf file and get the text of it. Then, apply regular expression to extract pids and their **reviews**, and stored in a dictionary. And tokenize the reviews for each **pid**'s **review**, store in another dictionary. After that, do the following process to output the correct ` vocab.txt` and `countVec.txt`\n",
    "\n",
    "[1] `case normalisation` for those tokens\n",
    "\n",
    "[2] Find ` top200 meaningful bigrams` by PMI measure\n",
    "\n",
    "[3] Remove `stopwords` and tokens which length`less than 3`\n",
    "\n",
    "[4] Remove `duplicated tokens` since we need to find the token frequency\n",
    "\n",
    "[5] Find `token frequency`\n",
    "\n",
    "[6] Remove `rare token` and `context_dependent` vocab\n",
    "\n",
    "[7] `Stemming` for unigrams and remove vocab which length`less than 3` and `stopwords`\n",
    "\n",
    "[8] Output `vocab.txt`\n",
    "\n",
    "[9] Find vocabs which appear in each pid and frequncy for each vocab\n",
    "\n",
    "[10] Create 'sparse matrix' by using `CountVector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSr_kwKclDEV"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acwZw2NklDEW"
   },
   "source": [
    "In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n",
    "\n",
    "* **os:** to interact with the operating system, e.g. navigate through folders to read files\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to work with dataframes\n",
    "* **multiprocessing:** to perform processes on multi cores for fast performance \n",
    "* **itertools:** to work with tokens\n",
    "* **nltk:** to use tokenizer and stemmer\n",
    "* **pdfminer:** to read pdf file\n",
    "* **io:** to read pdf file\n",
    "* **sklearn.feature_extraction.text:** to create CountVector\n",
    "* **math:** to calculate threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgmGWs8HlDEW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwNp0KnWlDEX"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "SA7fSJiRlDEY"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CJDLDI6lDEY"
   },
   "source": [
    "Let's examine what is the content of the file. For this purpose, PIDs is length of 10 and start with 'B0' or digital numbers. Also the review text start with `[` and end with `]`. However, the text content also include `[` and `]` like `[amazon]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ENnHWjoXlDEc"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9esGMx8lDEc"
   },
   "source": [
    "In this section, read pdf and get pid and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsnRR2c4lDEc"
   },
   "outputs": [],
   "source": [
    "# Function to trasfer pdf to text\n",
    "def pdf_to_text(path):\n",
    "    manager = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    layout = LAParams(all_texts=True)\n",
    "    device = TextConverter(manager, retstr, laparams=layout)\n",
    "    filepath = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(manager, device)\n",
    "\n",
    "    for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    filepath.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uh9oUXAlDEd"
   },
   "source": [
    "Let's examine the dictionary generated. For counting the total number of reviews extracted is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUZuFeuQlDEd"
   },
   "outputs": [],
   "source": [
    "# Reading pdf file \n",
    "pdf_text = pdf_to_text('32189222_task2.pdf')\n",
    "pdf_text\n",
    "\n",
    "pid_pattern = r'B0\\w{8}(?=\\n{2})|\\d{9}\\w(?=\\n{2})' # regular expression for extracting product ids\n",
    "pids = re.findall(pid_pattern, pdf_text)\n",
    "\n",
    "text_pattern = r'(?:\\n*\\[)(.*(?:\\n.*)*?\\n.*)(?:\\]\\n{2})' # regular expression for extracting reviews\n",
    "texts = re.findall(text_pattern, pdf_text)\n",
    "\n",
    "pid_text_dict = dict(zip(pids,texts))\n",
    "print(len(pids))\n",
    "print(len(texts))\n",
    "print(len(pid_text_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VIfQCD1VlDEe"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.1. Tokenization <a class=\"anchor\" name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gchByyjolDEf"
   },
   "source": [
    "Tokenization is a principal step in text processing and producing unigrams and bigrams. In this section, we need to do case normalization before we can do tokenization for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8zT4N0RlDEf"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "tokenized_dict = {}\n",
    "for k,v in pid_text_dict.items():\n",
    "    v = str(v).lower() # case normalization\n",
    "    tokenized_dict[k] = tokenizer.tokenize(v) # tokenization for texts\n",
    "tokenized_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqZos1q6lDEf"
   },
   "source": [
    "The above operation results in a dictionary with PID representing keys and a single string for all reviews of the day concatenated to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPBNTTq6lDEg"
   },
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tokenized_dict.values()))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVqFfwwMlDEg"
   },
   "source": [
    "`words` stores all the tokens in the pdf text.\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NglwwiJRnPZd"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.2. Whatever else <a class=\"anchor\" name=\"whetev\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ve6IZ2I-lDEg"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 4.3. Finding First 200 Meaningful Bigrams <a class=\"anchor\" name=\"bigrams\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erGhUY2UlDEg"
   },
   "source": [
    "One of the tasks is to find the first 200 meaningful bigrams. These bigrams should also be included in the final vocabulary list. And the top 200 meaningful bigrams should concatenate by `'_'` such as `abandonment_supposedly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgFFtm6qlDEg"
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "top200_bigrams = finder.nbest(bigram_measures.pmi, 200)\n",
    "top200_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cbSKT6PlDEg"
   },
   "source": [
    "Having found the top 200 meaningful bigrams, we need to retokenize tweets considering the bigrams as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hDrmwEylDEg"
   },
   "outputs": [],
   "source": [
    "# concatenate these bigrams by '_'\n",
    "bigrams = []\n",
    "for i in top200_bigrams:\n",
    "    bigrams.append(i[0] + '_' + i[1])\n",
    "bigrams\n",
    "# We need to keep the bigram instead of tokenize it, by using MWETokenzier\n",
    "mwe_tokenizer = MWETokenizer(top200_bigrams)\n",
    "bigram_dict= {}\n",
    "for k, v in tokenized_dict.items():\n",
    "    bigram_dict[k] = mwe_tokenizer.tokenize(v)\n",
    "bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words list\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "    stop_words = list(set(stop_words))                                              \n",
    "# Remove tokens which is stop word or length less than 3\n",
    "for k,v in bigram_dict.items():\n",
    "    bigram_dict[k] = [token for token in v if len(token) >= 3 and token not in stop_words]\n",
    "    \n",
    "# Remove duplicated tokens\n",
    "uniq_dict = {}\n",
    "for k,v in bigram_dict.items():\n",
    "    uniq_dict[k] = list(set(v))\n",
    "uniq_words = list(chain.from_iterable(uniq_dict.values()))\n",
    "uniq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the deduplicated token list, so we can get token frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of each token\n",
    "fd = FreqDist(uniq_words)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare token and context-dependent stopwords\n",
    "context_dependent = []\n",
    "rare_tokens = []\n",
    "for w, f in fd.items():\n",
    "    if f > math.ceil(len(pids)/2) and w not in bigrams:\n",
    "        context_dependent.append(w)\n",
    "    if f < 10 and w not in bigrams:\n",
    "        rare_tokens.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list for context dependent and rare tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for word in uniq_words:\n",
    "    # Remove context dependent words and rare tokens\n",
    "    if word not in context_dependent and word not in rare_tokens:\n",
    "        vocab.append(word)\n",
    "vocab = list(set(vocab))\n",
    "vocab.sort()\n",
    "\n",
    "# Stemming for the vocab\n",
    "stemmer = PorterStemmer()\n",
    "# vocab = list(set([stemmer.stem(w) for w in vocab]))\n",
    "stemmed_vocab = []\n",
    "for w in vocab:\n",
    "    if '_' not in w: # do not stemming for bigrams as stemming will change the meaning of bigrams\n",
    "        stemmed_vocab.append(stemmer.stem(w))\n",
    "    else:\n",
    "        stemmed_vocab.append(w)\n",
    "stemmed_vocab = list(set(stemmed_vocab)) # remove duplicated vocab\n",
    "stemmed_vocab = [w for w in stemmed_vocab if len(w)>= 3 and w not in stop_words] #\n",
    "stemmed_vocab.sort()\n",
    "stemmed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do stemming for the vocab, since stemming will impact our bigram, the stemming will be done by now. Also, we need to filter stemmed vocab again in case some stemmed vocab is stop word or length less than 3. At this stage, vocabs are found and ready to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already have the vocab list, we can start to check what vocabs are in the product reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict = {}\n",
    "for k,v in bigram_dict.items(): # the reason we use tokens before deduplication is we want to record frequency of the vocabs\n",
    "    vec_dict[k] = []\n",
    "    for w in v:\n",
    "        if w not in context_dependent and w not in rare_tokens and '_' not in w and len(stemmer.stem(w)) >= 3 and stemmer.stem(w) not in stop_words:\n",
    "            vec_dict[k].append(stemmer.stem(w))\n",
    "# dictionary that store pids and vocabs in its revew\n",
    "vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index_dict = {}\n",
    "for i in enumerate(stemmed_vocab):\n",
    "    vocab_index_dict[i[1]] = i[0]\n",
    "vocab_index_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the vocabs and their index, we can match vocabs with their index for each pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict_final = {}\n",
    "for k,v in vec_dict.items():\n",
    "    vec_dict_final[k] = []\n",
    "    for w in v:\n",
    "        vec_dict_final[k].append(vocab_index_dict[w])\n",
    "vec_dict_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary store pids and vocabs index, now we are ready to use CountVector to get vocabs' frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO1PJO-dlDEh"
   },
   "source": [
    "At this stage, we can output the vocab.txt and countVector.txt\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmaGJYIJlDEl"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjMBqRetlDEl"
   },
   "source": [
    "Four files need to be generated:\n",
    "* Vocabulary list\n",
    "* Sparse matrix\n",
    "\n",
    "This is performed in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc6tQ4ljlDEm"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.1. Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDlbpGYilDEm"
   },
   "source": [
    "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. By using enuerate, we can get words and their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6OUXHlxlDEm"
   },
   "outputs": [],
   "source": [
    "with open ('32189222_vocab.txt', 'w') as f:\n",
    "    for i in enumerate(stemmed_vocab):\n",
    "        f.write(i[1]+ ':' + str(i[0]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkGH81YFlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### 5.2. Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtxqUAwmlDEn"
   },
   "source": [
    "For writing sparse matrix for each PID, we firstly calculate the frequency of words for that PID and for each PID write the words' index and their frequency by using CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__n1fdIqlDEn"
   },
   "outputs": [],
   "source": [
    "with open('32189222_countVec.txt', 'w') as w:\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\") # each word is a feature\n",
    "    for k ,v in vec_dict_final.items():\n",
    "        v = [str(i) for i in v ]\n",
    "        data_features = vectorizer.fit_transform([' '.join(v)])\n",
    "        name_features = vectorizer.get_feature_names()\n",
    "        w.write(k + ',')\n",
    "        for word, count in zip(name_features, data_features.toarray()[0]):\n",
    "            if word != name_features[-1]:\n",
    "                w.write(word + ':' +str(count)+',')\n",
    "            else:\n",
    "                w.write(word + ':' +str(count))\n",
    "        w.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUFQU-QXlDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWjri6x_lDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXAprlSblDEn"
   },
   "source": [
    "The task is to create vocab and sparse matrix for pdf files. We used pdfminer to read pdf file and regular expressions to extract pids and reviews. Then do the following step to generate vocab.txt and sparse matrix.\n",
    "\n",
    "[1] `case normalisation` for those tokens\n",
    "\n",
    "[2] Find ` top200 meaningful bigrams` by PMI measure\n",
    "\n",
    "[3] Remove `stopwords` and tokens which length`less than 3`\n",
    "\n",
    "[4] Remove `duplicated tokens` since we need to find the token frequency\n",
    "\n",
    "[5] Find `token frequency`\n",
    "\n",
    "[6] Remove `rare token` and `context_dependent` vocab\n",
    "\n",
    "[7] `Stemming` for unigrams and remove vocab which length`less than 3` and `stopwords`\n",
    "\n",
    "[8] Output `vocab.txt`\n",
    "\n",
    "[9] Find vocabs which appear in each pid and frequncy for each vocab\n",
    "\n",
    "[10] Create 'sparse matrix' by using `CountVector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFXYKxO8lDEn"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HppxDtWNlDEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## 7. References <a class=\"anchor\" name=\"Ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCkWr-M1lDEo"
   },
   "source": [
    "[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 27/08/2022.\n",
    "\n",
    "[2] sklearn.feature_extraction.text.CountVectorizer, https://scikitlearn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer, Accessed 29/08/2022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp9O-a1UlDEo"
   },
   "source": [
    "\n",
    "## --------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
